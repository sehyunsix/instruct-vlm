{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3331b9bf-26fb-4f8b-bdbf-6c1e77e8173f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebf2000654842f4870ef5861b47f3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The image depicts a serene beach scene during what appears to be either sunrise or sunset, as indicated by the warm, golden light illuminating the sky and casting long shadows on the sand. A woman is sitting on the sandy beach, wearing a plaid shirt and dark pants, with her legs crossed. She has long hair and is smiling warmly at a light-colored dog, possibly a Labrador Retriever, which is sitting in front of her. The dog is wearing a harness and is extending its paw towards the woman's hand, suggesting a playful interaction between them. The ocean is visible in the background, with gentle waves rolling onto the shore\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93aed526-66d3-4cd0-a728-308267362174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484b521-4fd9-42fd-8e86-052855cd68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " test_df = pd.read_parquet('data/deep_chal_multitask_dataset_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8791d223-e185-4cd5-be03-0cf6f2a754de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_type</th>\n",
       "      <th>task</th>\n",
       "      <th>input</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image</td>\n",
       "      <td>captioning</td>\n",
       "      <td>https://pulpcovers.com/wp-content/uploads/2020...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image</td>\n",
       "      <td>captioning</td>\n",
       "      <td>https://pulpcovers.com/wp-content/uploads/2010...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image</td>\n",
       "      <td>captioning</td>\n",
       "      <td>https://pulpcovers.com/wp-content/uploads/2020...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image</td>\n",
       "      <td>captioning</td>\n",
       "      <td>https://pulpcovers.com/wp-content/uploads/2011...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image</td>\n",
       "      <td>captioning</td>\n",
       "      <td>https://pulpcovers.com/wp-content/uploads/2023...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>text</td>\n",
       "      <td>text_qa</td>\n",
       "      <td>Alan worked in an office in the city. He worke...</td>\n",
       "      <td>Why was Alan going to the farm to begin with?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>text</td>\n",
       "      <td>text_qa</td>\n",
       "      <td>The kitchen comes alive at night in the Sander...</td>\n",
       "      <td>Does she believe him?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>text</td>\n",
       "      <td>text_qa</td>\n",
       "      <td>A440 or A4 (also known as the Stuttgart pitch)...</td>\n",
       "      <td>What is one instrument A4 is used to tune?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>text</td>\n",
       "      <td>text_qa</td>\n",
       "      <td>The dog, called Prince, was an intelligent ani...</td>\n",
       "      <td>Who found it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>text</td>\n",
       "      <td>text_qa</td>\n",
       "      <td>Las Vegas (, Spanish for \"The Meadows\"), offic...</td>\n",
       "      <td>Which state is it in?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     input_type        task  \\\n",
       "0         image  captioning   \n",
       "1         image  captioning   \n",
       "2         image  captioning   \n",
       "3         image  captioning   \n",
       "4         image  captioning   \n",
       "...         ...         ...   \n",
       "2495       text     text_qa   \n",
       "2496       text     text_qa   \n",
       "2497       text     text_qa   \n",
       "2498       text     text_qa   \n",
       "2499       text     text_qa   \n",
       "\n",
       "                                                  input  \\\n",
       "0     https://pulpcovers.com/wp-content/uploads/2020...   \n",
       "1     https://pulpcovers.com/wp-content/uploads/2010...   \n",
       "2     https://pulpcovers.com/wp-content/uploads/2020...   \n",
       "3     https://pulpcovers.com/wp-content/uploads/2011...   \n",
       "4     https://pulpcovers.com/wp-content/uploads/2023...   \n",
       "...                                                 ...   \n",
       "2495  Alan worked in an office in the city. He worke...   \n",
       "2496  The kitchen comes alive at night in the Sander...   \n",
       "2497  A440 or A4 (also known as the Stuttgart pitch)...   \n",
       "2498  The dog, called Prince, was an intelligent ani...   \n",
       "2499  Las Vegas (, Spanish for \"The Meadows\"), offic...   \n",
       "\n",
       "                                           question  \n",
       "0                                                    \n",
       "1                                                    \n",
       "2                                                    \n",
       "3                                                    \n",
       "4                                                    \n",
       "...                                             ...  \n",
       "2495  Why was Alan going to the farm to begin with?  \n",
       "2496                          Does she believe him?  \n",
       "2497     What is one instrument A4 is used to tune?  \n",
       "2498                                  Who found it?  \n",
       "2499                          Which state is it in?  \n",
       "\n",
       "[2500 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bf48077-ef9f-4c11-9e6e-4a274d109a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:33,  2.44s/it]/workspace/.venv/lib/python3.11/site-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (97915360 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "69it [06:17,  5.48s/it]\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7fbb5a226700>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnidentifiedImageError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      4\u001b[39m     messages=[{\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m         ],\n\u001b[32m     13\u001b[39m     }]\n\u001b[32m     14\u001b[39m     text = processor.apply_chat_template(\n\u001b[32m     15\u001b[39m     messages, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     image_inputs, video_inputs = \u001b[43mprocess_vision_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     inputs = processor(\n\u001b[32m     19\u001b[39m         text=[text],\n\u001b[32m     20\u001b[39m         images=image_inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     )\n\u001b[32m     25\u001b[39m     inputs = inputs.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.11/site-packages/qwen_vl_utils/vision_process.py:330\u001b[39m, in \u001b[36mprocess_vision_info\u001b[39m\u001b[34m(conversations)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vision_info \u001b[38;5;129;01min\u001b[39;00m vision_infos:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mimage_url\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m         image_inputs.append(\u001b[43mfetch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvision_info\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvideo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vision_info:\n\u001b[32m    332\u001b[39m         video_inputs.append(fetch_video(vision_info))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.11/site-packages/qwen_vl_utils/vision_process.py:91\u001b[39m, in \u001b[36mfetch_image\u001b[39m\u001b[34m(ele, size_factor)\u001b[39m\n\u001b[32m     89\u001b[39m     image_obj = image\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m image.startswith(\u001b[33m\"\u001b[39m\u001b[33mhttp://\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m image.startswith(\u001b[33m\"\u001b[39m\u001b[33mhttps://\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     image_obj = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m image.startswith(\u001b[33m\"\u001b[39m\u001b[33mfile://\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     93\u001b[39m     image_obj = Image.open(image[\u001b[32m7\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.11/site-packages/PIL/Image.py:3580\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3578\u001b[39m     warnings.warn(message)\n\u001b[32m   3579\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[32m-> \u001b[39m\u001b[32m3580\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[31mUnidentifiedImageError\u001b[39m: cannot identify image file <_io.BytesIO object at 0x7fbb5a226700>"
     ]
    }
   ],
   "source": [
    "\n",
    "results=[]\n",
    "# short_test_df= test_df.iloc[:10]\n",
    "for index, row in tqdm(test_df.iterrows()):\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": row['input_type'],\n",
    "                row['input_type']: row['input'],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Do the task : \"+ row['task']},\n",
    "        ],\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    results.append({\"ID\" : index , \"Output\":output_text})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad097c13-6663-4ffa-b5c0-6fd268bd08d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:24,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing vision info for row 69: cannot identify image file <_io.BytesIO object at 0x7fbb3a599d00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [00:34,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing vision info for row 88: cannot identify image file <_io.BytesIO object at 0x7fbb3a999d00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "268it [02:01,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing vision info for row 267: cannot identify image file <_io.BytesIO object at 0x7fbb3a932430>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "358it [02:41,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing vision info for row 357: cannot identify image file <_io.BytesIO object at 0x7fbb3a998f40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "378it [02:53,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing vision info for row 377: cannot identify image file <_io.BytesIO object at 0x7fbb3a9324d0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "533it [03:50, 25.03it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "564it [03:51, 44.23it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "627it [03:51, 83.35it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "661it [03:52, 82.49it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "710it [03:53, 76.59it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "740it [03:53, 73.61it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "797it [03:54, 94.00it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "835it [03:54, 90.37it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "867it [03:55, 76.09it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "898it [03:55, 74.04it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "953it [03:56, 97.57it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "984it [03:56, 83.79it/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "2500it [04:00, 10.40it/s] \n"
     ]
    }
   ],
   "source": [
    "batches = []\n",
    "for index, row in tqdm(test_df.iterrows()):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": row['input_type'],\n",
    "                row['input_type']: row['input'],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Do the task : \" + row['task']},\n",
    "        ],\n",
    "    }]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    try:\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing vision info for row {index}: {str(e)}\")\n",
    "        messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": row['input'],\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Do the task : \" + row['task']},\n",
    "        ],\n",
    "        }]\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batches.append(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9dcf2d6c-be64-4127-abf6-a6fb4e7aac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# short_test_df= test_df.iloc[:10]\n",
    "for index, batch in tqdm(enumerate(batches):\n",
    "    inputs=batch\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    results.append({\"ID\" : index , \"Output\":output_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71669548-d186-48f0-98d0-43327e510c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1250 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 1/1250 [00:06<2:11:43,  6.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 2/1250 [00:14<2:34:31,  7.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 3/1250 [00:22<2:40:00,  7.70s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 4/1250 [00:35<3:22:03,  9.73s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 5/1250 [00:39<2:42:12,  7.82s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  0%|          | 5/1250 [00:53<3:43:27, 10.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m results = \u001b[43mprocess_batch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mprocess_batch_data\u001b[39m\u001b[34m(test_df, batch_size, processor, model, device)\u001b[39m\n\u001b[32m     51\u001b[39m inputs = inputs.to(device)\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Inference: Generate outputs for the batch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Trim generated IDs to remove input tokens\u001b[39;00m\n\u001b[32m     57\u001b[39m generated_ids_trimmed = [\n\u001b[32m     58\u001b[39m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids):] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)\n\u001b[32m     59\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2633\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2625\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2626\u001b[39m         input_ids=input_ids,\n\u001b[32m   2627\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2628\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2629\u001b[39m         **model_kwargs,\n\u001b[32m   2630\u001b[39m     )\n\u001b[32m   2632\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2633\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2644\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2645\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2646\u001b[39m         input_ids=input_ids,\n\u001b[32m   2647\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2648\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2649\u001b[39m         **model_kwargs,\n\u001b[32m   2650\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3605\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3602\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3603\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3605\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3606\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m   3607\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   3609\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def process_batch_data(test_df, batch_size, processor, model, device=\"cuda\"):\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the dataframe in batches\n",
    "    for batch_start in tqdm(range(0, len(test_df), batch_size)):\n",
    "        batch_end = min(batch_start + batch_size, len(test_df))\n",
    "        batch_df = test_df.iloc[batch_start:batch_end]\n",
    "        \n",
    "        # Prepare batch messages\n",
    "        batch_messages = []\n",
    "        batch_indices = []\n",
    "        for index, row in batch_df.iterrows():\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": row['input_type'],\n",
    "                        row['input_type']: row['input'],\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": \"Do the task : \" + row['task']},\n",
    "                ],\n",
    "            }]\n",
    "            batch_messages.append(messages)\n",
    "            batch_indices.append(index)\n",
    "        \n",
    "        # Apply chat template to all messages in the batch\n",
    "        batch_texts = [\n",
    "            processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            for messages in batch_messages\n",
    "        ]\n",
    "        \n",
    "        # Process vision inputs for the batch\n",
    "        batch_image_inputs = []\n",
    "        batch_video_inputs = []\n",
    "        for messages in batch_messages:\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            batch_image_inputs.append(image_inputs)\n",
    "            batch_video_inputs.append(video_inputs)\n",
    "        \n",
    "        # Prepare inputs for the model\n",
    "        inputs = processor(\n",
    "            text=batch_texts,\n",
    "            images=batch_image_inputs if any(batch_image_inputs) else None,\n",
    "            videos=batch_video_inputs if any(batch_video_inputs) else None,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Inference: Generate outputs for the batch\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        # Trim generated IDs to remove input tokens\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        # Decode the outputs\n",
    "        output_texts = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        \n",
    "        # Collect results\n",
    "        for idx, output_text in zip(batch_indices, output_texts):\n",
    "            results.append({\"ID\": idx, \"Output\": output_text})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "results = process_batch_data(test_df, batch_size=2, processor=processor, model=model, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sehyun",
   "language": "python",
   "name": "sehyun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
