{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71a5406-8517-487d-bc5e-628208d89588",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (/workspace/.venv/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, AutoModelForCausalLM, AdamW, get_scheduler\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AdamW' from 'transformers' (/workspace/.venv/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import torch\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba92734d-3d1d-4a82-aca3-be722bab7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    \"\"\"\n",
    "    하나의 데이터 샘플(row)을 Qwen 모델의 대화 형식으로 변환합니다.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are a helpful multimodal assistant. Your task is to follow the user's instructions carefully and provide an accurate response based on the provided image and/or text.\"\n",
    "    \n",
    "    # 태스크에 따라 유저 프롬프트를 동적으로 생성\n",
    "    task = sample['task']\n",
    "    question = sample.get('question', '')\n",
    "    context = sample['input'] if sample['input_type'] == 'text' else ''\n",
    "    \n",
    "    user_prompt = f\"Task: {task}\\n\"\n",
    "    if context:\n",
    "        user_prompt += f\"Context: {context}\\n\"\n",
    "    if pd.notna(question) and question:\n",
    "        user_prompt += f\"Question: {question}\\n\"\n",
    "    \n",
    "    user_prompt = user_prompt.strip()\n",
    "\n",
    "    # 이미지 처리\n",
    "    image = None\n",
    "    if sample['input_type'] == 'image':\n",
    "        try:\n",
    "            image = sample['input'] # URL을 그대로 전달\n",
    "        except Exception:\n",
    "            image = None # 오류 발생 시 이미지 없음\n",
    "\n",
    "    # 최종 대화 구조 생성\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": user_prompt},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"output\"]}],\n",
    "        },\n",
    "    ]\n",
    "    # 이미지 없는 샘플에서 이미지 노드 제거\n",
    "    if not image:\n",
    "        message[1]['content'] = [item for item in message[1]['content'] if item['type'] != 'image']\n",
    "        \n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25378980-8e8e-4bc6-9a5a-f4e607ac9d90",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Dataset' has no attribute 'from_pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hf_dataset = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pandas\u001b[49m(df)\n\u001b[32m      2\u001b[39m formatted_dataset = hf_dataset.map(\u001b[38;5;28;01mlambda\u001b[39;00m sample: {\u001b[33m'\u001b[39m\u001b[33mformatted_data\u001b[39m\u001b[33m'\u001b[39m: format_data(sample)}, remove_columns=\u001b[38;5;28mlist\u001b[39m(df.columns))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 데이터 분할 (예시)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: type object 'Dataset' has no attribute 'from_pandas'"
     ]
    }
   ],
   "source": [
    "hf_dataset = Dataset.from_pandas(df)\n",
    "formatted_dataset = hf_dataset.map(lambda sample: {'formatted_data': format_data(sample)}, remove_columns=list(df.columns))\n",
    "\n",
    "# 데이터 분할 (예시)\n",
    "train_test_split = formatted_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fafcf-f2a9-4b1f-a6cc-8e1fefc2bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 학습 인자 (SFTConfig) 설정 ---\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"qwen_vl_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False, # `formatted_data`를 사용하므로 이 옵션이 중요\n",
    "    dataset_text_field=\"formatted_data\", # TRL에게 포맷팅된 데이터가 어디 있는지 알려줌\n",
    "    # max_seq_length=2048, # 필요 시 시퀀스 길이 제한\n",
    ")\n",
    "\n",
    "\n",
    "# --- 7. SFTTrainer 초기화 ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# --- 8. 학습 시작 ---\n",
    "print(\"\\n파인튜닝을 시작합니다...\")\n",
    "trainer.train()\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "# --- 9. 모델 저장 ---\n",
    "print(\"최적의 모델을 저장합니다...\")\n",
    "trainer.save_model(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "print(f\"모델과 프로세서가 '{training_args.output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh",
   "language": "python",
   "name": "sh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
