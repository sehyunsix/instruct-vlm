{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2768491e-e89b-4aa1-bd88-46af1a836cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a401c-2f64-4ad5-b337-36f344d72128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71a5406-8517-487d-bc5e-628208d89588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fe56c5c88548d0bfe5ec3f6804edfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cb7bba0e4f4b97858e24e922b051c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e95c9b93e844c77901ca74d578eafb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cebe87373864f6da7b411d36a84ae8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19e9381235b42b08c15173484159407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e161f5668f1431ea6ef0c86467d3f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8276eac58124331836ce88f7d1e1a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577303c12b924d59b81e9471cfbe6faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d76d1a727947f99710853cd5e1cbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9958dc0735864b17b4b764216b9a1f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e57b6c420b433da9c5df7924608c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790e761e44194d798a6129ac999fcbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48de59ff56fb4e13a73616a366b0633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f8400433874f6eae0f5f554f2d590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc14e517e28c4d86aa4691ee4def3e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b93739275d94b8e8afc4635c79e2af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1280*28*28\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba92734d-3d1d-4a82-aca3-be722bab7b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    \"\"\"\n",
    "    하나의 데이터 샘플(row)을 Qwen 모델의 대화 형식으로 변환합니다.\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are a helpful multimodal assistant. Your task is to follow the user's instructions carefully and provide an accurate response based on the provided image and/or text.\"\n",
    "    \n",
    "    # 태스크에 따라 유저 프롬프트를 동적으로 생성\n",
    "    task = sample['task']\n",
    "    question = sample.get('question', '')\n",
    "    context = sample['input'] if sample['input_type'] == 'text' else ''\n",
    "    \n",
    "    user_prompt = f\"Task: {task}\\n\"\n",
    "    if context:\n",
    "        user_prompt += f\"Context: {context}\\n\"\n",
    "    if pd.notna(question) and question:\n",
    "        user_prompt += f\"Question: {question}\\n\"\n",
    "    \n",
    "    user_prompt = user_prompt.strip()\n",
    "\n",
    "    # 이미지 처리\n",
    "    image = None\n",
    "    if sample['input_type'] == 'image':\n",
    "        try:\n",
    "            image = sample['input'] # URL을 그대로 전달\n",
    "        except Exception:\n",
    "            image = None # 오류 발생 시 이미지 없음\n",
    "\n",
    "    # 최종 대화 구조 생성\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": user_prompt},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"output\"]}],\n",
    "        },\n",
    "    ]\n",
    "    # 이미지 없는 샘플에서 이미지 노드 제거\n",
    "    if not image:\n",
    "        message[1]['content'] = [item for item in message[1]['content'] if item['type'] != 'image']\n",
    "        \n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25378980-8e8e-4bc6-9a5a-f4e607ac9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_parquet('data/deep_chal_multitask_dataset.parquet')\n",
    "hf_dataset = Dataset.from_pandas(df)\n",
    "formatted_dataset = hf_dataset.map(lambda sample: {'formatted_data': format_data(sample)}, remove_columns=list(df.columns))\n",
    "# 데이터 분할 (예시)\n",
    "train_test_split = formatted_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fafcf-f2a9-4b1f-a6cc-8e1fefc2bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 학습 인자 (SFTConfig) 설정 ---\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"qwen_vl_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False, # `formatted_data`를 사용하므로 이 옵션이 중요\n",
    "    dataset_text_field=\"formatted_data\", # TRL에게 포맷팅된 데이터가 어디 있는지 알려줌\n",
    "    # max_seq_length=2048, # 필요 시 시퀀스 길이 제한\n",
    ")\n",
    "\n",
    "\n",
    "# --- 7. SFTTrainer 초기화 ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# --- 8. 학습 시작 ---\n",
    "print(\"\\n파인튜닝을 시작합니다...\")\n",
    "trainer.train()\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "# --- 9. 모델 저장 ---\n",
    "print(\"최적의 모델을 저장합니다...\")\n",
    "trainer.save_model(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "print(f\"모델과 프로세서가 '{training_args.output_dir}'에 저장되었습니다.\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh",
   "language": "python",
   "name": "sh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
